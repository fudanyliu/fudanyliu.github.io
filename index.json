
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a Ph.D. candidate in computer science at Fudan University, supervised by Prof.¬†Liang Song, and currently also a visiting scholar with the University of Toronto, working with by Prof. Igor Gilitschenski. My research interests include signal processing and pattern recognition, focusing on online evolutive learning and anomaly detection.\nüî•I am looking for postdoctoral position in AI-related fields such as statistical ML and generative AI and the applications in industry, healthcare and environment science.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. candidate in computer science at Fudan University, supervised by Prof.¬†Liang Song, and currently also a visiting scholar with the University of Toronto, working with by Prof.","tags":null,"title":"Yang Liu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://fudanyliu.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Yang Liu"],"categories":[],"content":"I will be co-organizer of ‚ÄúOnline Evolutive Learning in Intelligent Internet of Things‚Äù workshop at the 10th IEEE World Forum on Internet of Things with Prof. Song, Prof. Plataniotis, Prof. Victor C.M. Leung and Prof. Liu, focusing on the convergence of online evolutionary learning and the smart Internet.\n","date":1722511094,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722511094,"objectID":"7c012cb2d48a0704b3a3e406b7087a12","permalink":"https://fudanyliu.github.io/post/2408-oel/","publishdate":"2024-08-01T19:18:14+08:00","relpermalink":"/post/2408-oel/","section":"post","summary":"Invited to be a co-organizer of the OEL-IIoT workshop.","tags":[],"title":"Co-organizer of OEL-IIoT Workshop","type":"post"},{"authors":["Yang Liu","Dingkang Yang","Yan Wang","Jing Liu","Jun Liu","Azzedine Boukerche","Peng Sun","Liang Song"],"categories":null,"content":"","date":1712620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712620800,"objectID":"177f7d067f08c8945ed2596a2aea5085","permalink":"https://fudanyliu.github.io/publication/csur23-gvaed/","publishdate":"2024-04-09T00:00:00Z","relpermalink":"/publication/csur23-gvaed/","section":"publication","summary":"This review extends the narrow VAED concept from unsupervised video anomaly detection to Generalized Video Anomaly Event Detection, which provides a comprehensive survey that integrates recent works based on different assumptions and learning frameworks into an intuitive taxonomy.","tags":[],"title":"Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models","type":"publication"},{"authors":["Yang Liu"],"categories":[],"content":"I arrived in Toronto and was honored to join the Intelligent Systems Group led by Prof. Igor in the Department of Computer Science, University of Toronto. I look forward to a successful visiting Ph.D. career here and doing some interesting and meaningful work with all of you, wonderful and kind partners!\n","date":1701256694,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701256694,"objectID":"feb8f90f7f3d87408223dbc6d0eebffb","permalink":"https://fudanyliu.github.io/post/2311-tout/","publishdate":"2023-11-29T19:18:14+08:00","relpermalink":"/post/2311-tout/","section":"post","summary":"A new adventure in Toronto and UofT begins.","tags":[],"title":"Arrival at UofT","type":"post"},{"authors":["Yang Liu"],"categories":[],"content":"I was awarded the 2023 Fudan University Oceanwide Scholar (only 10 graduates).\n","date":1701249494,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701249494,"objectID":"a32e6186d4cd276d97c6c951f28a25b0","permalink":"https://fudanyliu.github.io/post/2311-fanhai/","publishdate":"2023-11-29T17:18:14+08:00","relpermalink":"/post/2311-fanhai/","section":"post","summary":"I was awarded the 2023 FDU Oceanwide Scholar.","tags":[],"title":"Oceanwide Scholar","type":"post"},{"authors":["Yang Liu","Dingkang Yang","Gaoyun Fang","Yuzheng Wang","Donglai Wei","Mengyang Zhao","Kai Cheng","Jing Liu","Liang Song"],"categories":null,"content":"","date":1700870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700870400,"objectID":"4e87cbf8a76caaea6112a110e63439c7","permalink":"https://fudanyliu.github.io/publication/kbs23-svn/","publishdate":"2023-11-25T00:00:00Z","relpermalink":"/publication/kbs23-svn/","section":"publication","summary":"The proposed Stochastic Video Normality (SVN) network learns the prototypical local appearance patterns via deterministic multi-task learning and global motion patterns in a non-deterministic manner.","tags":[],"title":"Stochastic Video Normality Network for Abnormal Event Detection in Surveillance Videos","type":"publication"},{"authors":["Yang Liu","Zhaoyang Xia","Liang Song"],"categories":null,"content":"","date":1698364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698364800,"objectID":"943ecf2c7be896f8f9f3e61cef6389ba","permalink":"https://fudanyliu.github.io/publication/mm23-crc/","publishdate":"2023-10-27T00:00:00Z","relpermalink":"/publication/mm23-crc/","section":"publication","summary":"We design a causality-inspired representation consistency (CRC) framework to implicitly learn the unobservable causal variables of normality directly from available normal videos and detect abnormal events with the learned representation consistency.","tags":[],"title":"Learning Causality-inspired Representation Consistency for Video Anomaly Detection","type":"publication"},{"authors":["Yang Liu"],"categories":[],"content":"I was awarded the 2023 national scholarship for PhD students at Fudan University.\n","date":1696151894,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696151894,"objectID":"ba52374d14e5d6661fbf580fd64fadd3","permalink":"https://fudanyliu.github.io/post/2309-natsch/","publishdate":"2023-10-01T17:18:14+08:00","relpermalink":"/post/2309-natsch/","section":"post","summary":"I was awarded the National Scholarship","tags":[],"title":"National Scholarship","type":"post"},{"authors":["Yang Liu","Jing Liu","Liang Song"],"categories":null,"content":"","date":1693353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693353600,"objectID":"927c36a51cde3d57d2daff874f47e9ed","permalink":"https://fudanyliu.github.io/publication/tii23-amp/","publishdate":"2023-08-30T00:00:00Z","relpermalink":"/publication/tii23-amp/","section":"publication","summary":"We propose an appearance-motion prototype network (AMP-net) that uses external memories to record prototype features and augments the appearance-motion prototype with a spatial-temporal fusion.","tags":[],"title":"AMP-Net: Appearance-Motion Prototype Network Assisted Automatic Video Anomaly Detection System","type":"publication"},{"authors":["Jing Liu","Yang Liu","Liang Song"],"categories":null,"content":"","date":1693008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693008000,"objectID":"ab95c9b6fd85696945f0e9e9c545659b","permalink":"https://fudanyliu.github.io/publication/pr23/","publishdate":"2023-08-26T00:00:00Z","relpermalink":"/publication/pr23/","section":"publication","summary":"We introduce a novel parallel model named Distributional and Spatial-Temporal Robust Representation (DSTRR), which combines automatic learning of statistical, spatial, and temporal features into a unified framework.","tags":[],"title":"Distributional and Spatial-Temporal Robust Representation Learning for Transportation Activity Recognition","type":"publication"},{"authors":null,"categories":null,"content":"Our research was conducted on Chongming Smart Island in Shanghai, where we utilized the concept of Vehicle to Everything (V2X) communication for autonomous vehicle experiments. This enables vehicles to receive real-time road and perceptual information, which can help improve road safety, increase operational efficiency, and reduce traffic accidents. The study mainly focuses on various aspects of vehicle-road coordination for autonomous driving, conducting experiments and testing its feasibility.\nFigure (a) shows our roadside perception integration device. This equipment can be used to collect environmental perception information from roadside, which can improve the safety and intelligence of autonomous vehicles as it provides real-time data on vehicles and pedestrians within a 100-meter range to assist in vehicle perception for surrounding vehicles and pedestrians.\nFigure (b) describes the autonomous logistics vehicles we used in our experiments. These vehicles can be used for human or cargo transportation within the park, and can operate without human intervention to improve transportation efficiency.\nFigure (c) displays the autonomous driving test vehicle. This vehicle is equipped with various advanced sensors such as lidar, millimeter-wave, and cameras that enable automatic driving through its own sensors or assisted by roadside sensing devices via V2X devices mounted on the roof.\nOur research aims to develop and improve current autonomous vehicle technology while promoting its widespread adoption. We hope to contribute to the development of autonomous driving technology while promoting improvements in road safety and transportation efficiency.\n","date":1614643200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614643200,"objectID":"139bc8319e0517e2058caabd3caee00d","permalink":"https://fudanyliu.github.io/research/chongming/","publishdate":"2021-03-02T00:00:00Z","relpermalink":"/research/chongming/","section":"research","summary":"This research conducted experiments on Chongming Smart Island utilizing V2X communication for autonomous vehicles, focusing on vehicle-road coordination and testing its feasibility.","tags":["CV","AD"],"title":"Automated driving with vehicle-road cooperation","type":"research"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"cdf2a558c00714467af10f2ba7729d27","permalink":"https://fudanyliu.github.io/project/changan/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/changan/","section":"project","summary":"Changan Project.","tags":["Deep Learning"],"title":"Changan Project","type":"project"}]